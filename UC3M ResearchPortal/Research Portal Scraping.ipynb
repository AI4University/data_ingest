{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d75386",
   "metadata": {},
   "source": [
    "# Import libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3da59e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "import json  \n",
    "from pathlib import Path\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# import langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b8c6fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.chrome.service import Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175319ef",
   "metadata": {},
   "source": [
    "# Download all researchers from Research Portal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffd546a",
   "metadata": {},
   "source": [
    "This step was executed in the terminal to download all the resources locally, avoiding looping over the same resources repeteadly and getting the access denied to the Research Portal.\n",
    "\n",
    "*Further information about this procedure can be found in the README.txt.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e84c274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseURL = 'https://researchportal.uc3m.es/display/inv'\n",
    "baseURL_activity = 'https://researchportal.uc3m.es/display/act'\n",
    "relative_path = '/Users/lcsanchez/Desktop/Research/researchportal.uc3m.es/'\n",
    "not_found_txt = 'Individual not found'\n",
    "valid_ids = []\n",
    "valid_acts_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b7014c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Researchers so far: 0 (10015)\r"
     ]
    }
   ],
   "source": [
    "for idx in np.arange(10000, 50000): \n",
    "    idx_str = str(idx)\n",
    "    portal_page = requests.get(baseURL+idx_str)\n",
    "    if portal_page.ok:\n",
    "        valid_ids.append(idx_str)\n",
    "        with open(relative_path + 'display/inv' + idx_str + '.html', 'w') as fout:\n",
    "            fout.write(portal_page.text)\n",
    "    print('Researchers so far: ' + str(len(valid_ids)) + ' (' + idx_str + ')\\r', end='')\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6159d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(relative_path + 'all_inv.txt', 'w') as fout:\n",
    "    fout.write('\\n'.join(valid_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8464808",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in np.arange(100000, 500000): \n",
    "    idx_str = str(idx)\n",
    "    portal_page = requests.get(baseURL_activity+idx_str)\n",
    "    if portal_page.ok:\n",
    "        valid_acts_ids.append(idx_str)\n",
    "        with open(relative_path + 'display/act' + idx_str + '.html', 'w') as fout:\n",
    "            fout.write(portal_page.text)\n",
    "    print('Activities so far: ' + str(len(valid_acts_ids)) + ' (' + idx_str + ')\\r', end='')\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b87eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(relative_path + 'all_act.txt', 'w') as fout:\n",
    "    fout.write('\\n'.join(valid_acts_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791dd299",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3193e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(relative_path + 'all_inv.txt', 'r') as f:\n",
    "    inv_ids = f.read().splitlines()\n",
    "    \n",
    "with open(relative_path + 'all_act.txt', 'r') as f:\n",
    "    inv_act = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961c72ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_act"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d3e64c",
   "metadata": {},
   "source": [
    "# Loading the driver and browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d61a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "# open it, go to a website, and get results\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver2 = webdriver.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda301cd",
   "metadata": {},
   "source": [
    "# Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5c5f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "researchers = []\n",
    "publications = []\n",
    "projects = []\n",
    "\n",
    "# Dataset researcherID - publicationID\n",
    "inv_pub = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d138eff0",
   "metadata": {},
   "source": [
    "## Researchers' information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4be64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access each url given the ID\n",
    "for n, inv in enumerate(inv_ids):\n",
    "    print(f'Researcher {n+1} out of {len(inv_ids)}')\n",
    "    inv_file = Path.cwd().joinpath(f'researchportal.uc3m.es/display/inv{inv}.html')\n",
    "    auth_url = f'file:///{inv_file}'\n",
    "    driver.get(auth_url)\n",
    "    \n",
    "# SCRAPING RESEARCHERS' INFORMATION\n",
    "    try:\n",
    "        name = driver.find_element(By.XPATH,'//span[@itemprop=\"name\"]')\n",
    "    except:\n",
    "        name = ''\n",
    "\n",
    "    try:\n",
    "        cat = driver.find_element(By.CLASS_NAME, 'categoriainv').text.split(': ')[1]\n",
    "    except:\n",
    "        cat = ''\n",
    "        \n",
    "    try:\n",
    "        orcid = driver.find_element(By.CLASS_NAME, 'individual-orcid').find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "    except:\n",
    "        orcid = ''\n",
    "\n",
    "    try:\n",
    "        scopus = driver.find_element(By.ID, 'scopusId-noRangeClass-List').find_element(By.TAG_NAME, 'a').text\n",
    "    except:\n",
    "        scopus = ''    \n",
    "    \n",
    "    try:\n",
    "        pos = driver.find_elements(By.CLASS_NAME, 'currentPosition')\n",
    "    except:\n",
    "        pos = []\n",
    "    \n",
    "    # Position: Academic department, Research group, Institute, ...\n",
    "    positions = []\n",
    "    for p in pos:\n",
    "            group = p.text.split(' : ')\n",
    "            pos_type = group[0]\n",
    "            if len(group) > 1:\n",
    "                pos_name = group[1]\n",
    "            else:\n",
    "                pos_name = ''\n",
    "            # link = p.find_element_by_tag_name('a').get_attribute('href')\n",
    "\n",
    "            positions.append((pos_type, pos_name))\n",
    "\n",
    "    try:\n",
    "        subject = driver.find_element(By.ID, 'individual-hasResearchArea').text\n",
    "    except:\n",
    "        subject = ''\n",
    "    \n",
    "    # Create an array for which each subject is an element\n",
    "    subjects = subject.splitlines()\n",
    "\n",
    "            \n",
    "    try:\n",
    "        email = driver.find_element(By.CLASS_NAME, 'individual-emails').find_element(By.TAG_NAME,'a').text\n",
    "    except:\n",
    "        email = ''\n",
    "       \n",
    "    \n",
    "    researchers.append({'invID':inv, 'name' : name.text, 'orcid' : orcid, 'scopus': scopus, 'category':cat, 'email':email, 'positions':dict(positions), 'subjects': subjects})\n",
    "    \n",
    "    # Conditional check and savings (Periodic backups)\n",
    "    if not n%100:\n",
    "        with open(relative_path + 'outputs/researchers.json', 'w') as f:\n",
    "            json.dump(researchers, f, indent=4)\n",
    "            \n",
    "with open(relative_path + 'outputs/researchers.json', 'w') as f:\n",
    "    json.dump(researchers, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0056f75",
   "metadata": {},
   "source": [
    "## Publications' information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38beb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access each url given the ID\n",
    "for n, inv in enumerate(inv_act):\n",
    "    \n",
    "    print(f'\\nActivity {n+1} out of {len(inv_act)}')\n",
    "    inv_file = Path.cwd().joinpath(f'researchportal.uc3m.es/display/act{inv}.html')\n",
    "    auth_url = f'file:///{inv_file}'\n",
    "    driver.get(auth_url)\n",
    "    \n",
    "    # section: articles, book chapters, conference contributions, working papers, projects, ...\n",
    "    section = driver.find_element(By.CLASS_NAME, 'display-title').text\n",
    "    valid_sections = [\"Articles\", \"Book Chapters\", \"Conference Contributions\", \"Working Papers\"]\n",
    "            \n",
    "    try:\n",
    "        if section in valid_sections:\n",
    "            resID = inv\n",
    "            # print(\"Resource ID: \", resID)\n",
    "            \n",
    "            # Title of the activity\n",
    "            title = driver.find_element(By.CLASS_NAME, 'fn').text\n",
    "            for section_name in valid_sections:\n",
    "                if title.endswith(section_name):\n",
    "                    title = title.rsplit(section_name, 1)[0].strip()\n",
    "                    # print(\"Title: \", title)\n",
    "                    break  \n",
    "                    \n",
    "            # Publication date\n",
    "            try:         \n",
    "                publication_date = driver.find_element(By.XPATH, '//h3[@id=\"dateTimeValue\"]/following-sibling::ul/li').text.strip()\n",
    "                # print(\"Publication Date: \", publication_date)\n",
    "            except:\n",
    "                publication_date = \"\"\n",
    "                # print(\"Publication Date: \", publication_date)\n",
    "\n",
    "                \n",
    "            # Publisher/magazine\n",
    "            try: \n",
    "                if section == \"Articles\":\n",
    "                    publication_venue = driver.find_element(By.XPATH, '//h3[@id=\"hasPublicationVenue\"]/following-sibling::ul/li/a').text.strip()\n",
    "                    # print(\"Publication Venue: \", publication_venue)\n",
    "                    \n",
    "                elif section in [\"Book Chapters\", \"Conference Contributions\", \"Working Papers\"]:\n",
    "                    publication_venue = driver.find_element(By.XPATH, '//h3[@id=\"publisher\"]/following-sibling::ul/li/a').text.strip()\n",
    "                    # print(\"Publisher:\", publication_venue)\n",
    "                    \n",
    "            except:\n",
    "                publication_venue = \"\"\n",
    "                # print(\"Publisher:\", publication_venue)\n",
    "\n",
    "                    \n",
    "            # DOI number\n",
    "            try: \n",
    "                doi = driver.find_element(By.XPATH, '//h3[@id=\"doi\"]/following-sibling::ul/li/a').text.strip()\n",
    "                # print(\"DOI:\", doi)\n",
    "            except: \n",
    "                doi = \"\"\n",
    "                # print(\"DOI:\", doi)\n",
    "\n",
    "                \n",
    "            # Abstract\n",
    "            try:\n",
    "                abstract = driver.find_element(By.XPATH, '//h3[@id=\"abstract\"]/following-sibling::ul/li').text.strip()\n",
    "                # print(\"Abstract: \", abstract)\n",
    "            except:\n",
    "                abstract = \"\"\n",
    "                # print(\"Abstract: \", abstract)\n",
    "\n",
    "\n",
    "                \n",
    "            # Keywords\n",
    "            try:\n",
    "                keywords_list = []\n",
    "                keywords = driver.find_element(By.XPATH, '//h3[@id=\"freetextKeyword\"]/following-sibling::ul/li').text.strip()\n",
    "                # Split the string using commas\n",
    "                keywords_split_by_comma = keywords.split(',')\n",
    "\n",
    "                # Split each resulting keyword using semicolons\n",
    "                keywords_list = [keyword.strip() for keyword_with_semicolon in keywords_split_by_comma for keyword in keyword_with_semicolon.split(';')]\n",
    "                # print(\"Keywords List: \", keywords_list)\n",
    "            except:\n",
    "                keywords_list = []\n",
    "                # print(\"Keywords List: \", keywords_list)\n",
    "\n",
    "                \n",
    "            # Research Areas\n",
    "            try: \n",
    "                research_areas = driver.find_elements(By.XPATH, '//h3[@id=\"hasResearchArea\"]/following-sibling::ul/li')\n",
    "                research_areas = [element.text.strip() for element in research_areas]\n",
    "                # print(\"Research Areas: \", research_areas)\n",
    "            except:\n",
    "                research_areas = []\n",
    "                # print(\"Research Areas: \", research_areas)\n",
    "\n",
    "            \n",
    "            # Authors IDs (if there is any ID)\n",
    "            try:\n",
    "                # Locate the parent <article> element\n",
    "                article_element = driver.find_element(By.XPATH, '//article[@class=\"property\" and @role=\"article\"]')\n",
    "                # Locate the <ul> element within the article for authors\n",
    "                authors_list = article_element.find_element(By.XPATH, '//ul[@role=\"list\" and @id=\"relatedBy-Authorship-List\"]')\n",
    "\n",
    "                # Get all <li> elements within the authors list\n",
    "                author_items = authors_list.find_elements(By.XPATH, 'li')\n",
    "\n",
    "                # Extract the author IDs\n",
    "                author_ids = []\n",
    "\n",
    "                for author_order, author_item in enumerate(author_items, start=1):\n",
    "                    author_name = author_item.text.strip()\n",
    "                    # print(\"Author: \", author_name, \"with order:\", author_order)\n",
    "                    href_attribute = author_item.find_elements(By.XPATH, 'a')\n",
    "                    \n",
    "                    for invID in href_attribute:\n",
    "                        invID = invID.get_attribute('href').split(\"inv\")[1]\n",
    "                        author_ids.append(invID)\n",
    "\n",
    "                        inv_pub.append({'invID': invID, 'pubID': resID, 'orderID': author_order})\n",
    "        \n",
    "                # print(\"Valid Researchers IDs: \", author_ids)\n",
    "\n",
    "            except:\n",
    "                author_ids = []\n",
    "    \n",
    "            publications.append({'resID': resID, 'section': section, 'title': title, 'doi': doi, 'publication_date': publication_date, 'publisher': publication_venue, 'abstract': abstract, 'keywords': keywords_list, 'research_areas': research_areas})\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if not n%100:\n",
    "        with open(relative_path + 'outputs/publications.json', 'w') as f:\n",
    "            json.dump(publications, f, indent=4)\n",
    "        with open(relative_path + 'outputs/inv_pub.json', 'w') as f:\n",
    "            json.dump(inv_pub, f, indent=4)\n",
    "            \n",
    "with open(relative_path + 'outputs/publications.json', 'w') as f:\n",
    "    json.dump(publications, f, indent=4)\n",
    "with open(relative_path + 'outputs/inv_pub.json', 'w') as f:\n",
    "    json.dump(inv_pub, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4166533",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    # SCRAPING PROJECTS' INFORMATION\n",
    "    projects = []\n",
    "    try:\n",
    "        pub_list = driver.find_element(By.ID,'projectsGroup').find_elements(By.CLASS_NAME, 'property')\n",
    "        for section in pub_list:\n",
    "            # section_title: 'principal researcher on', 'researcher on'\n",
    "            section_title = section.find_element(By.TAG_NAME, 'h3').text\n",
    "            sections = []\n",
    "            for p in section.find_elements(By.TAG_NAME,'li'):\n",
    "                element = p.find_element(By.TAG_NAME, 'a')\n",
    "                # resource ID\n",
    "                resID = element.get_attribute('href').split('/')[-1][3:]\n",
    "                \n",
    "                try:\n",
    "                    inv_file = Path.cwd().joinpath(f'researchportal.uc3m.es/display/act{resID}.html')\n",
    "                    auth_url = f'file:///{inv_file}'\n",
    "                    driver2.get(auth_url)\n",
    "\n",
    "                    \n",
    "                    property_list = driver2.find_elements(By.CLASS_NAME, 'property')\n",
    "\n",
    "                    # Iterar a través de los elementos 'property' para encontrar el abstract\n",
    "                    for article in property_list:\n",
    "                        abstract = \"\"\n",
    "\n",
    "                        # Verificar si el ID del elemento contiene 'abstract'\n",
    "                        abstract_elements = article.find_elements(By.ID, \"abstract-noRangeClass-List\")\n",
    "                        if abstract_elements:\n",
    "                            # Extraer el texto del elemento\n",
    "                            abstract = abstract_elements[0].text\n",
    "                except:\n",
    "                    abstract = ''\n",
    "                    \n",
    "                title = element.text\n",
    "                year = p.find_element(By.TAG_NAME, 'span').text\n",
    "                \n",
    "                                \n",
    "                try:\n",
    "                    funding_entity = p.find_element(By.XPATH, './/a[@title=\"awarded by\"]').text\n",
    "                \n",
    "                except:\n",
    "                    funding_entity = \"\"\n",
    "                    \n",
    "                sections.append({'resID':resID, 'title':title, 'year':year, 'funding_entity': funding_entity, 'abstract': abstract})\n",
    "            projects.append((section_title, sections))\n",
    "    except:\n",
    "        pass\n",
    "    projects.append({'author':inv, 'projects':dict(projects)})\n",
    "  \n",
    "    \n",
    "    # Conditional check and savings (Periodic backups)\n",
    "    if not n%100:\n",
    "        with open(relative_path + 'outputs/researchers.json', 'w') as f:\n",
    "            json.dump(researchers, f, indent=4)\n",
    "        with open(relative_path + 'outputs/publications.json', 'w') as f:\n",
    "            json.dump(publications_data, f, indent=4)\n",
    "        with open(relative_path + 'outputs/projects.json', 'w') as f:\n",
    "            json.dump(projects, f, indent=4)\n",
    "\n",
    "# Always saving (Periodic backups)\n",
    "\n",
    "with open(relative_path + 'outputs/researchers.json', 'w') as f:\n",
    "    json.dump(researchers, f, indent=4)\n",
    "with open(relative_path + 'outputs/publications.json', 'w') as f:\n",
    "    json.dump(publications, f, indent=4)\n",
    "with open(relative_path + 'outputs/projects.json', 'w') as f:\n",
    "    json.dump(projects, f, indent=4) \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404888ac",
   "metadata": {},
   "source": [
    "# Process JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44498e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json(obj):\n",
    "    '''\n",
    "    Function to process json recursively\n",
    "    '''\n",
    "    resources = []\n",
    "    auth_res = []\n",
    "    author = ''\n",
    "    def process(obj, objType='', author=''):\n",
    "        if isinstance(obj, dict):\n",
    "            for k, v in obj.items():     \n",
    "                if k == 'author':\n",
    "                    author = v\n",
    "                if k == 'title':\n",
    "                    d = {'type':objType.strip()}\n",
    "                    d.update(obj)\n",
    "                    resources.append(d)\n",
    "                    auth_res.append((author, obj['resID']))\n",
    "                else:\n",
    "                    if isinstance(v, (dict, list)):\n",
    "                        process(v, k, author)\n",
    "        elif isinstance(obj, list):\n",
    "            for el in obj:\n",
    "                process(el, objType, author)\n",
    "    process(obj)\n",
    "    return resources, auth_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1628ea67",
   "metadata": {},
   "source": [
    "# Formating our dataset and completing abstracts with 'scopus' database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f61c462",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Regular expression to extract DOI from the URL\n",
    "doi_pattern = r'https://doi\\.org/(?:http://dx\\.doi\\.org/)?(.+)'\n",
    "\n",
    "# Extract DOI values from URLs in the 'doi' column\n",
    "df_resources['doi'] = df_resources['doi'].str.extract(doi_pattern)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d069652b",
   "metadata": {},
   "source": [
    "### Formating 'df_resources' database: homogenizing NAs and filtering observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd53fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Homogenizing the NAs to the same format\n",
    "# Lista de nombres de las columnas en las que deseas reemplazar los valores nulos o vacíos por NaN\n",
    "columns_to_process = ['doi', 'title', 'year', 'abstract', 'funding_entity', 'type']\n",
    "\n",
    "# Iterar sobre las columnas y reemplazar los valores nulos o vacíos por NaN\n",
    "for column in columns_to_process:\n",
    "    df_resources[column] = df_resources[column].apply(lambda x: np.nan if x in [None, ''] else x)\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc72d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Filter the dataset by observations that have at least doi or at least abstract (so, an observation that can be completed if it has no abstract)\n",
    "filtered_df_resources = df_resources[(df_resources['doi'].isna() & ~df_resources['abstract'].isna()) | (~df_resources['doi'].isna() & ~df_resources['abstract'].isna()) | (~df_resources['doi'].isna() & df_resources['abstract'].isna())]\n",
    "filtered_df_resources.reset_index(drop=True, inplace=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c75dc47",
   "metadata": {},
   "source": [
    "### Formating Scopus database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807db5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data = pd.read_parquet('/Users/lcsanchez/Desktop/Research/Scopus/scopus_data.parquet')\n",
    "\n",
    "filtered_data = data[['doi', 'description']]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87bbc1a",
   "metadata": {},
   "source": [
    "### Joining databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e830c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Perform a left join on 'doi' column\n",
    "merged_df = pd.merge(filtered_df_resources, filtered_data, on='doi', how='left')\n",
    "#merged_df\n",
    "\n",
    "# Llenar NaN en la columna 'abstract' con el valor del abstract de SCOPUS ('description' column) si 'abstract' está vacía\n",
    "merged_df['abstract'] = merged_df['abstract'].combine_first(merged_df['description'])\n",
    "\n",
    "# Eliminamos los duplicados\n",
    "merged_df.drop_duplicates(subset='resID', keep='first', inplace=True)\n",
    "\n",
    "merged_df = merged_df.drop(columns=['description'])\n",
    "\n",
    "# Save the merged table as a CSV file\n",
    "merged_df.to_csv(relative_path + 'outputs/merged_table.csv', index=False)\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
