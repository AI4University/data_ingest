{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d75386",
   "metadata": {},
   "source": [
    "# Import libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3da59e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "import json  \n",
    "from pathlib import Path\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "# import langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b8c6fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.chrome.service import Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175319ef",
   "metadata": {},
   "source": [
    "# Download all researchers from Research Portal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffd546a",
   "metadata": {},
   "source": [
    "This step was executed in the terminal to download all the resources locally, avoiding looping over the same resources repeteadly and getting the access denied to the Research Portal.\n",
    "\n",
    "*Further information about this procedure can be found in the README.txt.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e84c274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseURL = 'https://researchportal.uc3m.es/display/inv'\n",
    "baseURL_activity = 'https://researchportal.uc3m.es/display/act'\n",
    "relative_path = '/Users/lcsanchez/Desktop/Research/researchportal.uc3m.es/'\n",
    "not_found_txt = 'Individual not found'\n",
    "valid_ids = []\n",
    "valid_acts_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b7014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in np.arange(10000, 50000): \n",
    "    idx_str = str(idx)\n",
    "    portal_page = requests.get(baseURL+idx_str)\n",
    "    if portal_page.ok:\n",
    "        valid_ids.append(idx_str)\n",
    "        with open(relative_path + 'display/inv' + idx_str + '.html', 'w') as fout:\n",
    "            fout.write(portal_page.text)\n",
    "    print('Researchers so far: ' + str(len(valid_ids)) + ' (' + idx_str + ')\\r', end='')\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6159d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(relative_path + 'all_inv.txt', 'w') as fout:\n",
    "    fout.write('\\n'.join(valid_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8464808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activities so far: 2 (371444)\r"
     ]
    }
   ],
   "source": [
    "for idx in np.arange(371443, 371445): \n",
    "    idx_str = str(idx)\n",
    "    portal_page = requests.get(baseURL_activity+idx_str)\n",
    "    if portal_page.ok:\n",
    "        valid_acts_ids.append(idx_str)\n",
    "        with open(relative_path + 'display/act' + idx_str + '.html', 'w') as fout:\n",
    "            fout.write(portal_page.text)\n",
    "    print('Activities so far: ' + str(len(valid_acts_ids)) + ' (' + idx_str + ')\\r', end='')\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75b87eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(relative_path + 'all_act.txt', 'w') as fout:\n",
    "    fout.write('\\n'.join(valid_acts_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791dd299",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3193e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(relative_path + 'all_inv.txt', 'r') as f:\n",
    "    inv_ids = f.read().splitlines()\n",
    "    \n",
    "with open(relative_path + 'all_act.txt', 'r') as f:\n",
    "    inv_act = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "961c72ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['371443', '371444']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_act"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d3e64c",
   "metadata": {},
   "source": [
    "# Loading the driver and browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a03d61a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "# open it, go to a website, and get results\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver2 = webdriver.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda301cd",
   "metadata": {},
   "source": [
    "# Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c5c5f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "researchers = []\n",
    "publications = []\n",
    "projects = []\n",
    "\n",
    "# Dataset researcherID - publicationID\n",
    "inv_pub = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4be64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access each url given the ID\n",
    "for n, inv in enumerate(inv_ids):\n",
    "    print(f'Researcher {n+1} out of {len(inv_ids)}')\n",
    "    inv_file = Path.cwd().joinpath(f'researchportal.uc3m.es/display/inv{inv}.html')\n",
    "    auth_url = f'file:///{inv_file}'\n",
    "    driver.get(auth_url)\n",
    "    \n",
    "# SCRAPING RESEARCHERS' INFORMATION\n",
    "    try:\n",
    "        name = driver.find_element(By.XPATH,'//span[@itemprop=\"name\"]')\n",
    "    except:\n",
    "        name = ''\n",
    "\n",
    "    try:\n",
    "        cat = driver.find_element(By.CLASS_NAME, 'categoriainv').text.split(': ')[1]\n",
    "    except:\n",
    "        cat = ''\n",
    "        \n",
    "    try:\n",
    "        orcid = driver.find_element(By.CLASS_NAME, 'individual-orcid').find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "    except:\n",
    "        orcid = ''\n",
    "\n",
    "    try:\n",
    "        scopus = driver.find_element(By.ID, 'scopusId-noRangeClass-List').find_element(By.TAG_NAME, 'a').text\n",
    "    except:\n",
    "        scopus = ''    \n",
    "    \n",
    "    try:\n",
    "        pos = driver.find_elements(By.CLASS_NAME, 'currentPosition')\n",
    "    except:\n",
    "        pos = []\n",
    "    \n",
    "    # Position: Academic department, Research group, Institute, ...\n",
    "    positions = []\n",
    "    for p in pos:\n",
    "            group = p.text.split(' : ')\n",
    "            pos_type = group[0]\n",
    "            if len(group) > 1:\n",
    "                pos_name = group[1]\n",
    "            else:\n",
    "                pos_name = ''\n",
    "            # link = p.find_element_by_tag_name('a').get_attribute('href')\n",
    "\n",
    "            positions.append((pos_type, pos_name))\n",
    "\n",
    "    try:\n",
    "        subject = driver.find_element(By.ID, 'individual-hasResearchArea').text\n",
    "    except:\n",
    "        subject = ''\n",
    "    \n",
    "    # Create an array for which each subject is an element\n",
    "    subjects = subject.splitlines()\n",
    "\n",
    "            \n",
    "    try:\n",
    "        email = driver.find_element(By.CLASS_NAME, 'individual-emails').find_element(By.TAG_NAME,'a').text\n",
    "    except:\n",
    "        email = ''\n",
    "       \n",
    "    \n",
    "    researchers.append({'invID':inv, 'name' : name.text, 'orcid' : orcid, 'scopus': scopus, 'category':cat, 'email':email, 'positions':dict(positions), 'subjects': subjects})\n",
    "    \n",
    "    # Conditional check and savings (Periodic backups)\n",
    "    if not n%100:\n",
    "        with open(relative_path + 'outputs/researchers.json', 'w') as f:\n",
    "            json.dump(researchers, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b38beb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Activity 1 out of 2\n",
      "Resource ID:  371443\n",
      "Title:  Extending the concurrency model of the real-time specification for Java\n",
      "Publication Date:  September 2011\n",
      "Publication Venue:  CONCURRENCY AND COMPUTATION-PRACTICE & EXPERIENCE\n",
      "DOI: https://doi.org/10.1002/cpe.1675\n",
      "Abstract:  The current RTSJ (Real-Time Specification for Java) threading model is dualized: a programmer has to decide between the high predictability offered by the region-based model and the flexibility offered by the garbage collection. So far, there is no unique type of thread which offers both the high predictability of a non-heap thread and the flexibility of a real-time thread in a single entity. Furthermore, this lack has a serious impact on the programmer who has to deal with new and sometimes non-trivial to use mechanisms, such as specific queues of objects or new types of threads, in order to avoid the priority inversion caused by the garbage collector. In order to tackle the concern properly and provide an improved and more generalized programming model, the authors propose a simple extension to the current threading model named RealtimeThread++, in an attempt to introduce more flexibility in the RTSJ concurrency model. The paper describes the extension from several points of view: (i) the programmer, identifying scenarios that may benefit from it significantly; (ii) the real-time Java technology perspective, identifying changes required in the current real-time virtual machine to support it; and (iii) the accumulated experience, relating empirical results obtained from a software prototype that supports the extension.\n",
      "Keywords List:  []\n",
      "Research Areas:  []\n",
      "Author:  BASANTA VAL, PABLO with order: 1\n",
      "Author:  GARCIA VALLS, MARIA SOLEDAD with order: 2\n",
      "Author:  ESTEVEZ AYRES, IRIA MANUELA with order: 3\n",
      "Valid Researchers IDs:  ['21578']\n",
      "\n",
      "Activity 2 out of 2\n",
      "Resource ID:  371444\n",
      "Title:  An OCL-Based approach to derive constraint test cases for database applications\n",
      "Publication Date:  September 2011\n",
      "Publication Venue:  INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING\n",
      "DOI: \n",
      "Abstract:  Abstract: The development of database applications in most CASE tools has been insufficient because most of these tools do not provide the software necessary to validate these appli-cations. Validation means ensuring whether a given application fulfils the user require-ments. We suggest validation of database applications by using the functional testing technique, which is a fundamental black-box testing technique for checking the software without being concerned about its implementation and structure. Our main contribu-tion to this work is in providing a MDA approach for deriving testing software from the OCL specification of the integrity constraints. This testing software is used to validate the database applications, which are used to enforce these constraints. The generated testing software includes three components: validation queries, test cases and initial data inserted before the testing process. Our approach is implemented as an add-in tool in Rational Rose called OCL2TestSW.\n",
      "Keywords List:  ['functional testing software', 'software validation', 'equivalence class testing', 'mda', 'case tools']\n",
      "Research Areas:  ['Computer Science']\n",
      "Author:  CUADRA FERNANDEZ, MARIA DOLORES with order: 1\n",
      "Author:  ABDULLA JASSIM, HARITH AL JUMAILY with order: 2\n",
      "Author:  CASTRO GALAN, ELENA with order: 3\n",
      "Author:  VELASCO DE DIEGO, MANUEL with order: 4\n",
      "Valid Researchers IDs:  ['35641', '18070']\n"
     ]
    }
   ],
   "source": [
    "# Access each url given the ID\n",
    "for n, inv in enumerate(inv_act):\n",
    "    \n",
    "    print(f'\\nActivity {n+1} out of {len(inv_act)}')\n",
    "    inv_file = Path.cwd().joinpath(f'researchportal.uc3m.es/display/act{inv}.html')\n",
    "    auth_url = f'file:///{inv_file}'\n",
    "    driver.get(auth_url)\n",
    "    \n",
    "    # section: articles, book chapters, conference contributions, working papers, projects, ...\n",
    "    section = driver.find_element(By.CLASS_NAME, 'display-title').text\n",
    "    valid_sections = [\"Articles\", \"Book Chapters\", \"Conference Contributions\", \"Working Papers\"]\n",
    "            \n",
    "    try:\n",
    "        if section in valid_sections:\n",
    "            resID = inv\n",
    "            print(\"Resource ID: \", resID)\n",
    "            \n",
    "            # Title of the activity\n",
    "            title = driver.find_element(By.CLASS_NAME, 'fn').text\n",
    "            for section_name in valid_sections:\n",
    "                if title.endswith(section_name):\n",
    "                    title = title.rsplit(section_name, 1)[0].strip()\n",
    "                    print(\"Title: \", title)\n",
    "                    break  \n",
    "                    \n",
    "            # Publication date\n",
    "            try:         \n",
    "                publication_date = driver.find_element(By.XPATH, '//h3[@id=\"dateTimeValue\"]/following-sibling::ul/li').text.strip()\n",
    "                print(\"Publication Date: \", publication_date)\n",
    "            except:\n",
    "                publication_date = \"\"\n",
    "                print(\"Publication Date: \", publication_date)\n",
    "\n",
    "                \n",
    "            # Publisher/magazine\n",
    "            try: \n",
    "                if section == \"Articles\":\n",
    "                    publication_venue = driver.find_element(By.XPATH, '//h3[@id=\"hasPublicationVenue\"]/following-sibling::ul/li/a').text.strip()\n",
    "                    print(\"Publication Venue: \", publication_venue)\n",
    "                    \n",
    "                elif section in [\"Book Chapters\", \"Conference Contributions\", \"Working Papers\"]:\n",
    "                    publication_venue = driver.find_element(By.XPATH, '//h3[@id=\"publisher\"]/following-sibling::ul/li/a').text.strip()\n",
    "                    print(\"Publisher:\", publication_venue)\n",
    "                    \n",
    "            except:\n",
    "                publication_venue = \"\"\n",
    "                print(\"Publisher:\", publication_venue)\n",
    "\n",
    "                    \n",
    "            # DOI number\n",
    "            try: \n",
    "                doi = driver.find_element(By.XPATH, '//h3[@id=\"doi\"]/following-sibling::ul/li/a').text.strip()\n",
    "                print(\"DOI:\", doi)\n",
    "            except: \n",
    "                doi = \"\"\n",
    "                print(\"DOI:\", doi)\n",
    "\n",
    "                \n",
    "            # Abstract\n",
    "            try:\n",
    "                abstract = driver.find_element(By.XPATH, '//h3[@id=\"abstract\"]/following-sibling::ul/li').text.strip()\n",
    "                print(\"Abstract: \", abstract)\n",
    "            except:\n",
    "                abstract = \"\"\n",
    "                print(\"Abstract: \", abstract)\n",
    "\n",
    "\n",
    "                \n",
    "            # Keywords\n",
    "            try:\n",
    "                keywords_list = []\n",
    "                keywords = driver.find_element(By.XPATH, '//h3[@id=\"freetextKeyword\"]/following-sibling::ul/li').text.strip()\n",
    "                # Split the string using commas\n",
    "                keywords_split_by_comma = keywords.split(',')\n",
    "\n",
    "                # Split each resulting keyword using semicolons\n",
    "                keywords_list = [keyword.strip() for keyword_with_semicolon in keywords_split_by_comma for keyword in keyword_with_semicolon.split(';')]\n",
    "                print(\"Keywords List: \", keywords_list)\n",
    "            except:\n",
    "                keywords_list = []\n",
    "                print(\"Keywords List: \", keywords_list)\n",
    "\n",
    "                \n",
    "            # Research Areas\n",
    "            try: \n",
    "                research_areas = driver.find_elements(By.XPATH, '//h3[@id=\"hasResearchArea\"]/following-sibling::ul/li')\n",
    "                research_areas = [element.text.strip() for element in research_areas]\n",
    "                print(\"Research Areas: \", research_areas)\n",
    "            except:\n",
    "                research_areas = []\n",
    "                print(\"Research Areas: \", research_areas)\n",
    "\n",
    "            \n",
    "            # Authors IDs (if there is any ID)\n",
    "            try:\n",
    "                # Locate the parent <article> element\n",
    "                article_element = driver.find_element(By.XPATH, '//article[@class=\"property\" and @role=\"article\"]')\n",
    "                # Locate the <ul> element within the article for authors\n",
    "                authors_list = article_element.find_element(By.XPATH, '//ul[@role=\"list\" and @id=\"relatedBy-Authorship-List\"]')\n",
    "\n",
    "                # Get all <li> elements within the authors list\n",
    "                author_items = authors_list.find_elements(By.XPATH, 'li')\n",
    "\n",
    "                # Extract the author IDs\n",
    "                author_ids = []\n",
    "\n",
    "                for author_order, author_item in enumerate(author_items, start=1):\n",
    "                    author_name = author_item.text.strip()\n",
    "                    print(\"Author: \", author_name, \"with order:\", author_order)\n",
    "                    href_attribute = author_item.find_elements(By.XPATH, 'a')\n",
    "                    \n",
    "                    for invID in href_attribute:\n",
    "                        invID = invID.get_attribute('href').split(\"inv\")[1]\n",
    "                        author_ids.append(invID)\n",
    "\n",
    "                        inv_pub.append({'invID': invID, 'pubID': resID, 'orderID': author_order})\n",
    "        \n",
    "                print(\"Valid Researchers IDs: \", author_ids)\n",
    "\n",
    "            except:\n",
    "                author_ids = []\n",
    "    \n",
    "            publications.append({'resID': resID, 'section': section, 'title': title, 'doi': doi, 'publication_date': publication_date, 'publisher': publication_venue, 'abstract': abstract, 'keywords': keywords_list, 'research_areas': research_areas})\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if not n%100:\n",
    "        with open(relative_path + 'outputs/publications.json', 'w') as f:\n",
    "            json.dump(publications, f, indent=4)\n",
    "        with open(relative_path + 'outputs/inv_pub.json', 'w') as f:\n",
    "            json.dump(inv_pub, f, indent=4)\n",
    "            \n",
    "with open(relative_path + 'outputs/publications.json', 'w') as f:\n",
    "    json.dump(publications, f, indent=4)\n",
    "with open(relative_path + 'outputs/inv_pub.json', 'w') as f:\n",
    "    json.dump(inv_pub, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fbc3edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'invID': '21578', 'pubID': '371443', 'orderID': 3},\n",
       " {'invID': '35641', 'pubID': '371444', 'orderID': 2},\n",
       " {'invID': '18070', 'pubID': '371444', 'orderID': 4}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_pub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4166533",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    # SCRAPING PROJECTS' INFORMATION\n",
    "    projects = []\n",
    "    try:\n",
    "        pub_list = driver.find_element(By.ID,'projectsGroup').find_elements(By.CLASS_NAME, 'property')\n",
    "        for section in pub_list:\n",
    "            # section_title: 'principal researcher on', 'researcher on'\n",
    "            section_title = section.find_element(By.TAG_NAME, 'h3').text\n",
    "            sections = []\n",
    "            for p in section.find_elements(By.TAG_NAME,'li'):\n",
    "                element = p.find_element(By.TAG_NAME, 'a')\n",
    "                # resource ID\n",
    "                resID = element.get_attribute('href').split('/')[-1][3:]\n",
    "                \n",
    "                try:\n",
    "                    inv_file = Path.cwd().joinpath(f'researchportal.uc3m.es/display/act{resID}.html')\n",
    "                    auth_url = f'file:///{inv_file}'\n",
    "                    driver2.get(auth_url)\n",
    "\n",
    "                    \n",
    "                    property_list = driver2.find_elements(By.CLASS_NAME, 'property')\n",
    "\n",
    "                    # Iterar a través de los elementos 'property' para encontrar el abstract\n",
    "                    for article in property_list:\n",
    "                        abstract = \"\"\n",
    "\n",
    "                        # Verificar si el ID del elemento contiene 'abstract'\n",
    "                        abstract_elements = article.find_elements(By.ID, \"abstract-noRangeClass-List\")\n",
    "                        if abstract_elements:\n",
    "                            # Extraer el texto del elemento\n",
    "                            abstract = abstract_elements[0].text\n",
    "                except:\n",
    "                    abstract = ''\n",
    "                    \n",
    "                title = element.text\n",
    "                year = p.find_element(By.TAG_NAME, 'span').text\n",
    "                \n",
    "                                \n",
    "                try:\n",
    "                    funding_entity = p.find_element(By.XPATH, './/a[@title=\"awarded by\"]').text\n",
    "                \n",
    "                except:\n",
    "                    funding_entity = \"\"\n",
    "                    \n",
    "                sections.append({'resID':resID, 'title':title, 'year':year, 'funding_entity': funding_entity, 'abstract': abstract})\n",
    "            projects.append((section_title, sections))\n",
    "    except:\n",
    "        pass\n",
    "    projects.append({'author':inv, 'projects':dict(projects)})\n",
    "  \n",
    "    \n",
    "    # Conditional check and savings (Periodic backups)\n",
    "    if not n%100:\n",
    "        with open(relative_path + 'outputs/researchers.json', 'w') as f:\n",
    "            json.dump(researchers, f, indent=4)\n",
    "        with open(relative_path + 'outputs/publications.json', 'w') as f:\n",
    "            json.dump(publications_data, f, indent=4)\n",
    "        with open(relative_path + 'outputs/projects.json', 'w') as f:\n",
    "            json.dump(projects, f, indent=4)\n",
    "\n",
    "# Always saving (Periodic backups)\n",
    "\n",
    "with open(relative_path + 'outputs/researchers.json', 'w') as f:\n",
    "    json.dump(researchers, f, indent=4)\n",
    "with open(relative_path + 'outputs/publications.json', 'w') as f:\n",
    "    json.dump(publications, f, indent=4)\n",
    "with open(relative_path + 'outputs/projects.json', 'w') as f:\n",
    "    json.dump(projects, f, indent=4) \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404888ac",
   "metadata": {},
   "source": [
    "# Process JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44498e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json(obj):\n",
    "    '''\n",
    "    Function to process json recursively\n",
    "    '''\n",
    "    resources = []\n",
    "    auth_res = []\n",
    "    author = ''\n",
    "    def process(obj, objType='', author=''):\n",
    "        if isinstance(obj, dict):\n",
    "            for k, v in obj.items():     \n",
    "                if k == 'author':\n",
    "                    author = v\n",
    "                if k == 'title':\n",
    "                    d = {'type':objType.strip()}\n",
    "                    d.update(obj)\n",
    "                    resources.append(d)\n",
    "                    auth_res.append((author, obj['resID']))\n",
    "                else:\n",
    "                    if isinstance(v, (dict, list)):\n",
    "                        process(v, k, author)\n",
    "        elif isinstance(obj, list):\n",
    "            for el in obj:\n",
    "                process(el, objType, author)\n",
    "    process(obj)\n",
    "    return resources, auth_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ad57ba",
   "metadata": {},
   "source": [
    "### Create tables to match author ID (invID) to resource ID (resID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0495927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF researchers\n",
    "with open(relative_path + 'outputs/researchers.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    df_researchers = pd.json_normalize(data)\n",
    "\n",
    "df_researchers.columns = ['invID']+[cname.split('.')[-1].replace(' ', '_').lower() for cname in df_researchers.columns[1:]]\n",
    "\n",
    "# DF resources, DF auth-res\n",
    "resources = []\n",
    "auth_res = []\n",
    "auth_res_publications = []\n",
    "auth_res_projects = []\n",
    "auth_res_others = []\n",
    "\n",
    "with open(relative_path + 'outputs/publications.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "res, a_r = process_json(data)\n",
    "resources.extend(res)\n",
    "auth_res.extend(a_r)\n",
    "auth_res_publications.extend(a_r)\n",
    "\n",
    "# DF projects\n",
    "with open(relative_path + 'outputs/projects.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "res, a_r = process_json(data)\n",
    "resources.extend(res)\n",
    "auth_res.extend(a_r)\n",
    "auth_res_projects.extend(a_r)\n",
    "\n",
    "# DF Others\n",
    "with open(relative_path + 'outputs/others.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "res, a_r = process_json(data)\n",
    "resources.extend(res)\n",
    "auth_res.extend(a_r)\n",
    "auth_res_others.extend(a_r)\n",
    "\n",
    "df_resources = pd.DataFrame(resources)\n",
    "df_resources = df_resources.drop_duplicates(subset='resID')\n",
    "df_resources = df_resources[list(df_resources.columns[1:]) + [df_resources.columns[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a22ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auth_res = pd.DataFrame(auth_res, columns=['invID', 'resID'])\n",
    "df_auth_res_publications = pd.DataFrame(auth_res_publications, columns=['invID', 'resID'])\n",
    "df_auth_res_projects = pd.DataFrame(auth_res_projects, columns=['invID', 'resID'])\n",
    "df_auth_res_others = pd.DataFrame(auth_res_others, columns=['invID', 'resID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95288532",
   "metadata": {},
   "source": [
    "# Save as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f761bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_researchers.to_csv(relative_path + 'outputs/researchers.csv', index=False)\n",
    "df_resources.to_csv(relative_path + 'outputs/resources.csv', index=False)\n",
    "\n",
    "df_auth_res_publications.to_csv(relative_path + 'outputs/auth_res_publications.csv', index=False)\n",
    "df_auth_res_projects.to_csv(relative_path + 'outputs/auth_res_projects.csv', index=False)\n",
    "df_auth_res_others.to_csv(relative_path + 'outputs/auth_res_others.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6f28db",
   "metadata": {},
   "source": [
    "# Save as a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01472fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_researchers.to_parquet(relative_path + 'outputs/researchers.parquet', index=False)\n",
    "df_resources.to_parquet(relative_path + 'outputs/resources.parquet', index=False)\n",
    "df_auth_res.to_parquet(relative_path + '/outputs/auth_res.parquet', index=False)\n",
    "\n",
    "df_auth_res_publications.to_parquet(relative_path + 'outputs/auth_res_publications.parquet', index=False)\n",
    "df_auth_res_projects.to_parquet(relative_path + 'outputs/auth_res_projects.parquet', index=False)\n",
    "df_auth_res_others.to_parquet(relative_path + 'outputs/auth_res_others.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1628ea67",
   "metadata": {},
   "source": [
    "# Formating our dataset and completing abstracts with 'scopus' database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f61c462",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Regular expression to extract DOI from the URL\n",
    "doi_pattern = r'https://doi\\.org/(?:http://dx\\.doi\\.org/)?(.+)'\n",
    "\n",
    "# Extract DOI values from URLs in the 'doi' column\n",
    "df_resources['doi'] = df_resources['doi'].str.extract(doi_pattern)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d069652b",
   "metadata": {},
   "source": [
    "### Formating 'df_resources' database: homogenizing NAs and filtering observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd53fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Homogenizing the NAs to the same format\n",
    "# Lista de nombres de las columnas en las que deseas reemplazar los valores nulos o vacíos por NaN\n",
    "columns_to_process = ['doi', 'title', 'year', 'abstract', 'funding_entity', 'type']\n",
    "\n",
    "# Iterar sobre las columnas y reemplazar los valores nulos o vacíos por NaN\n",
    "for column in columns_to_process:\n",
    "    df_resources[column] = df_resources[column].apply(lambda x: np.nan if x in [None, ''] else x)\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc72d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Filter the dataset by observations that have at least doi or at least abstract (so, an observation that can be completed if it has no abstract)\n",
    "filtered_df_resources = df_resources[(df_resources['doi'].isna() & ~df_resources['abstract'].isna()) | (~df_resources['doi'].isna() & ~df_resources['abstract'].isna()) | (~df_resources['doi'].isna() & df_resources['abstract'].isna())]\n",
    "filtered_df_resources.reset_index(drop=True, inplace=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c75dc47",
   "metadata": {},
   "source": [
    "### Formating Scopus database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807db5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data = pd.read_parquet('/Users/lcsanchez/Desktop/Research/Scopus/scopus_data.parquet')\n",
    "\n",
    "filtered_data = data[['doi', 'description']]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87bbc1a",
   "metadata": {},
   "source": [
    "### Joining databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e830c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Perform a left join on 'doi' column\n",
    "merged_df = pd.merge(filtered_df_resources, filtered_data, on='doi', how='left')\n",
    "#merged_df\n",
    "\n",
    "# Llenar NaN en la columna 'abstract' con el valor del abstract de SCOPUS ('description' column) si 'abstract' está vacía\n",
    "merged_df['abstract'] = merged_df['abstract'].combine_first(merged_df['description'])\n",
    "\n",
    "# Eliminamos los duplicados\n",
    "merged_df.drop_duplicates(subset='resID', keep='first', inplace=True)\n",
    "\n",
    "merged_df = merged_df.drop(columns=['description'])\n",
    "\n",
    "# Save the merged table as a CSV file\n",
    "merged_df.to_csv(relative_path + 'outputs/merged_table.csv', index=False)\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
